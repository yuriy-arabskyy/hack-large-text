{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 DSPy RAG Optimization\n",
    "\n",
    "This notebook uses the golden QA dataset to optimize the DSPy RAG retriever.\n",
    "\n",
    "## Steps:\n",
    "1. Load golden QA dataset\n",
    "2. Set up DSPy with OpenAI model\n",
    "3. Create evaluation metric\n",
    "4. Load existing RAG agent\n",
    "5. Run baseline evaluation\n",
    "6. Optimize retriever with DSPy compiler\n",
    "7. Evaluate optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuriyarabskyy/Library/Caches/pypoetry/virtualenvs/hack-large-text-ASlkIg2G-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "from hack.rag_agent import create_agent\n",
    "from hack.retriever import FaissRetriever\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Golden QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 QA pairs\n",
      "\n",
      "Example QA pair:\n",
      "Q: Why does the simple move described in the passage ruin Black's plan and lead to the eventual loss of his pawns?\n",
      "A: Because the move prevents Black's pieces from coordinating, leaving his pawns weak and vulnerable, so he will eventually lose them.\n",
      "Context: A simple move, which destroys Black's plan utterly. Black will now have no concerted action of his p...\n"
     ]
    }
   ],
   "source": [
    "# Load golden QA dataset\n",
    "with open('golden_qa_with_answers.json', 'r') as f:\n",
    "    golden_qa = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(golden_qa)} QA pairs\")\n",
    "print(f\"\\nExample QA pair:\")\n",
    "print(f\"Q: {golden_qa[0]['question']}\")\n",
    "print(f\"A: {golden_qa[0]['answer']}\")\n",
    "print(f\"Context: {golden_qa[0]['context'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert to DSPy Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 35 examples\n",
      "Validation set: 15 examples\n"
     ]
    }
   ],
   "source": [
    "# Convert golden QA to DSPy examples\n",
    "# Split into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(golden_qa, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Train set: {len(train_data)} examples\")\n",
    "print(f\"Validation set: {len(val_data)} examples\")\n",
    "\n",
    "# Create DSPy examples\n",
    "train_examples = [\n",
    "    dspy.Example(\n",
    "        question=qa['question'],\n",
    "        answer=qa['answer'],\n",
    "        context=qa['context']\n",
    "    ).with_inputs('question')\n",
    "    for qa in train_data\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    dspy.Example(\n",
    "        question=qa['question'],\n",
    "        answer=qa['answer'],\n",
    "        context=qa['context']\n",
    "    ).with_inputs('question')\n",
    "    for qa in val_data\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up DSPy with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DSPy with OpenAI\n",
    "lm = dspy.LM('openai/gpt-4o-mini', api_key=openai_api_key)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with 1941 vectors\n",
      "Loaded 1941 blocks from workspace\n",
      "Agent created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create RAG agent with FAISS retriever\n",
    "agent = create_agent(\n",
    "    faiss_index_path=\"chess_pdf.faiss\",\n",
    "    workspace_json_path=\"workspace_with_embeddings.json\"\n",
    ")\n",
    "\n",
    "print(\"Agent created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Evaluation Metric\n",
    "\n",
    "We'll use a simple metric that checks if the answer contains key information from the golden answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_correctness_metric(example, prediction, trace=None):\n",
    "    \"\"\"\n",
    "    Evaluate answer quality by checking if key concepts from golden answer\n",
    "    are present in the predicted answer.\n",
    "    \n",
    "    Returns a score between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Get predicted answer\n",
    "    if isinstance(prediction, dict):\n",
    "        pred_answer = prediction.get('answer', '')\n",
    "    else:\n",
    "        pred_answer = str(prediction)\n",
    "    \n",
    "    golden_answer = example.answer.lower()\n",
    "    pred_answer = pred_answer.lower()\n",
    "    \n",
    "    # Simple keyword-based scoring\n",
    "    # Extract important words (remove common words)\n",
    "    stop_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "                  'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'but', 'it', 'as',\n",
    "                  'that', 'this', 'with', 'from', 'by', 'so', 'if', 'not', 'can', 'will'}\n",
    "    \n",
    "    golden_words = set(w for w in golden_answer.split() if w not in stop_words and len(w) > 3)\n",
    "    pred_words = set(w for w in pred_answer.split() if w not in stop_words and len(w) > 3)\n",
    "    \n",
    "    if not golden_words:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap = len(golden_words & pred_words)\n",
    "    score = overlap / len(golden_words)\n",
    "    \n",
    "    return min(1.0, score)\n",
    "\n",
    "\n",
    "def retrieval_recall_metric(example, prediction, trace=None):\n",
    "    \"\"\"\n",
    "    Check if the golden context appears in retrieved candidates.\n",
    "    This evaluates retriever quality.\n",
    "    \"\"\"\n",
    "    if trace is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Look for retrieved candidates in trace\n",
    "    # This is a simplified check - you may need to adjust based on your trace structure\n",
    "    golden_context = example.context.lower()\n",
    "    \n",
    "    # Check if context substring appears in any retrieved text\n",
    "    # This is a proxy for checking if the right chunk was retrieved\n",
    "    if hasattr(prediction, '__dict__'):\n",
    "        for key, value in prediction.__dict__.items():\n",
    "            if isinstance(value, str) and golden_context[:50] in value.lower():\n",
    "                return 1.0\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def combined_metric(example, prediction, trace=None):\n",
    "    \"\"\"\n",
    "    Combined metric: 50% answer correctness + 50% retrieval recall.\n",
    "    \"\"\"\n",
    "    answer_score = answer_correctness_metric(example, prediction, trace)\n",
    "    retrieval_score = retrieval_recall_metric(example, prediction, trace)\n",
    "    return 0.5 * answer_score + 0.5 * retrieval_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:45:47 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline RAG agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:45:47 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n",
      "2025/10/02 15:45:47 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n",
      "2025/10/02 15:45:48 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: 0.38\n",
      "Example 2: 0.60\n",
      "Example 3: 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:47:19 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 4: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:48:26 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 5: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:50:36 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 6: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:50:37 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 7: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:50:43 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 8: 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:51:19 WARNING dspy.primitives.module: Calling module.forward(...) on WorkspaceAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 9: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/02 15:52:53 WARNING dspy.adapters.json_adapter: Failed to use structured output format, falling back to JSON mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on example 10: litellm.RateLimitError: RateLimitError: OpenAIException - Rate limit reached for gpt-4o-mini in organization org-j6MkLqiZgm6JZAieTnFaC1x4 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.\n",
      "\n",
      "Baseline average score: 0.493\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline performance on validation set\n",
    "print(\"Evaluating baseline RAG agent...\")\n",
    "\n",
    "baseline_scores = []\n",
    "for i, example in enumerate(val_examples[:10]):  # Start with subset\n",
    "    try:\n",
    "        prediction = agent.forward(example.question)\n",
    "        score = answer_correctness_metric(example, prediction)\n",
    "        baseline_scores.append(score)\n",
    "        print(f\"Example {i+1}: {score:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on example {i+1}: {e}\")\n",
    "        baseline_scores.append(0.0)\n",
    "\n",
    "baseline_avg = sum(baseline_scores) / len(baseline_scores) if baseline_scores else 0.0\n",
    "print(f\"\\nBaseline average score: {baseline_avg:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimize with DSPy\n",
    "\n",
    "We'll use DSPy's BootstrapFewShot optimizer to improve the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = BootstrapFewShot(\n",
    "    metric=answer_correctness_metric,\n",
    "    max_bootstrapped_demos=4,\n",
    "    max_labeled_demos=4,\n",
    "    max_rounds=1\n",
    ")\n",
    "\n",
    "print(\"Starting optimization...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "# Compile/optimize the agent\n",
    "optimized_agent = optimizer.compile(\n",
    "    agent,\n",
    "    trainset=train_examples[:20],  # Start with subset\n",
    "    # valset=val_examples[:10]\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Optimized Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized performance\n",
    "print(\"Evaluating optimized RAG agent...\")\n",
    "\n",
    "optimized_scores = []\n",
    "for i, example in enumerate(val_examples[:10]):\n",
    "    try:\n",
    "        prediction = optimized_agent.forward(example.question)\n",
    "        score = answer_correctness_metric(example, prediction)\n",
    "        optimized_scores.append(score)\n",
    "        print(f\"Example {i+1}: {score:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error on example {i+1}: {e}\")\n",
    "        optimized_scores.append(0.0)\n",
    "\n",
    "optimized_avg = sum(optimized_scores) / len(optimized_scores) if optimized_scores else 0.0\n",
    "print(f\"\\nOptimized average score: {optimized_avg:.3f}\")\n",
    "print(f\"Baseline average score: {baseline_avg:.3f}\")\n",
    "print(f\"Improvement: {(optimized_avg - baseline_avg):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Specific Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs optimized on a specific example\n",
    "test_example = val_examples[0]\n",
    "\n",
    "print(\"Question:\", test_example.question)\n",
    "print(\"\\nGolden Answer:\", test_example.answer)\n",
    "\n",
    "baseline_pred = agent.forward(test_example.question)\n",
    "print(\"\\n--- Baseline Prediction ---\")\n",
    "print(baseline_pred.get('answer', 'No answer'))\n",
    "\n",
    "optimized_pred = optimized_agent.forward(test_example.question)\n",
    "print(\"\\n--- Optimized Prediction ---\")\n",
    "print(optimized_pred.get('answer', 'No answer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Optimized Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized agent\n",
    "optimized_agent.save('optimized_rag_agent.json')\n",
    "print(\"Optimized agent saved to optimized_rag_agent.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Full Validation Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on full validation set (optional - may be slow)\n",
    "print(\"Running full validation set evaluation...\")\n",
    "\n",
    "full_baseline_scores = []\n",
    "full_optimized_scores = []\n",
    "\n",
    "for i, example in enumerate(val_examples):\n",
    "    print(f\"\\rProcessing example {i+1}/{len(val_examples)}\", end=\"\")\n",
    "    \n",
    "    try:\n",
    "        # Baseline\n",
    "        baseline_pred = agent.forward(example.question)\n",
    "        baseline_score = answer_correctness_metric(example, baseline_pred)\n",
    "        full_baseline_scores.append(baseline_score)\n",
    "        \n",
    "        # Optimized\n",
    "        optimized_pred = optimized_agent.forward(example.question)\n",
    "        optimized_score = answer_correctness_metric(example, optimized_pred)\n",
    "        full_optimized_scores.append(optimized_score)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError on example {i+1}: {e}\")\n",
    "        full_baseline_scores.append(0.0)\n",
    "        full_optimized_scores.append(0.0)\n",
    "\n",
    "print(\"\\n\\n=== Full Validation Results ===\")\n",
    "print(f\"Baseline average: {sum(full_baseline_scores)/len(full_baseline_scores):.3f}\")\n",
    "print(f\"Optimized average: {sum(full_optimized_scores)/len(full_optimized_scores):.3f}\")\n",
    "print(f\"Improvement: {(sum(full_optimized_scores)-sum(full_baseline_scores))/len(full_baseline_scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-large-text-ASlkIg2G-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
